#+title: Sequetial Vector Benchmark
#+author: Sholde
#+date: 2021

* Script
** Installation

   The =install.sh= script provide all information about the system.

   Take *1 argument*:
   - cpu architecture, to create a directory for follow information.

   In the following lines, we supposed that we are on
   =cpu_architecture/= directory.

   Compiler information are on =compiler/= directory.
   See =compiler/gcc_version.txt=.

   *glibc* information are on =lib/= directory.
   See =lib/glibc_version.txt=.

   *cpu*, *caches* and *memory* information are on =system/=
   directory.
   - dmidecode: =system/hw.txt=
   - cpu: =system/cpu/info.txt=
   - caches: =system/caches/all.txt=
   - memory: =system/memory/info.txt=
   - numa: =system/numa/info.txt=
   - lstopo: =system/topo/lstopo.png=

** Compilation
   
   The =compile.sh= compile all programs.
   Executables are in their source directory.

** Execution
   
   The =run.sh= execute all programs and store their output on file.

   It take *exactly* 6 arguments in entry:
   1. cpu architecture (to create a folder and store data)
   2. core id (taskset on the program on this core)
   3. size to fit in L1 cache
   4. size to fit in L2 cache
   5. size to fit in L3 cache
   6. size to fit in DRAM

*** More details about the script

    After checking arguments, run all programs by number of array
    they use. Here we are at most *2 arrays*. Therefore the script
    run programs which use *1 array*, then programs use *2 arrays*.
    Run all benchmark with 1,000 iteration for more accuracy.

    Store all output in the directory: =cpu_architecture/benchmark/=.
    _For example:_ if we specify *knl* cpu architecture, the *reduc*
    benchmark output will be on =knl/benchmark/reduc/= directory.

    *IMPORTANT NOTE:* It store *BRUT* output in *.txt* files and
    plotting data in *.dat* files.

    We can easily see the standart derivation and others information
    of all benchmark by *cat* the file.

** Plotting

   Plotting step are separate in 2 files:
   - a bash file
   - a gnuplot file

*** Bash file

    Main plotting script.
    
    Take *3 arguments*:
    1. a delta (to avoid plotting with max y)
    2. cpu architecture (without spaces)
    3. the name of cpu


    Plotting into *png* file on the same directory of benchmark data.

*** GNUplot file

    Not supposed to be called, but you can. The bash script above call
    this *gnuplot* script.

    _WARNING_: it return a *pdf* file on output.

    Take *3 arguments*:
    1. a delta (to avoid plotting with max y)
    2. path of data file + name of benchmark (i.e: knl/benchmark/reduc/reduc)
    3. the name of benchmark (without spaces)
    4. the name of cpu

    Chech the max bandwidth of L1 cache and take it on max y range for
    others caches and DRAM.

** Clean

   Clean the follow list of file :
   - temporary file (*~)
   - executable file

* Benchmark
** Coffee Lake
*** Information

    I run all benchmark on a *USB LIVE Manjaro XFCE* with 4G.

    #+CAPTION: Topology of Intel Core i5-8400
    #+NAME: fig:topology_INTEL_CORE_I5-8400
    #+ATTR_HTML: :width 1500px
    [[./coffee_lake/system/topology/lstopo.png]]

    Caches size:
    - L1: 32K
    - L2: 256K
    - L3: 9216K
    - DRAM: 8G

    I decide to run benchmarks with :
    - L1: 24K
    - L2: 200K
    - L3: 5000K
    - DRAM: 15000K

    This size are chosen because they fit on their caches/ram.

    The governor was set to *performance*, because *userspace* wasn't
    available.
    And the frequency was set to *4.00GHz*. (the maximum)

**** Command

#+begin_src bash
$ ./install.sh coffee_lake
$ ./compile.sh
$ sudo cpupower -c 5 frequency-set --governor performance -f 4.00GHz
$ ./run.sh coffee_lake 5 24 200 5000 15000
$ ./plot.sh 10 coffee_lake "Intel Core i5-8400"
#+end_src

*** copy

    #+CAPTION: Copy benchmark on an Intel Core i5-8400
    #+NAME: fig:load_INTEL_CORE_I5-8400
    #+ATTR_HTML: :width 1500px
    [[./coffee_lake/benchmark/copy/copy_bw.png]]

    We can see a difference of bandwith for *sse* and *avx* for L1
    cache, where *avx* is better but for other memory space there are
    not difference.

    Also, all *sse* size is the same.

*** dotprod

    #+CAPTION: Dotprod benchmark on an Intel Core i5-8400
    #+NAME: fig:load_INTEL_CORE_I5-8400
    #+ATTR_HTML: :width 1500px
    [[./coffee_lake/benchmark/dotprod/dotprod_bw.png]]

    For *L1* cache we can see an evolution between all size. *AVX*
    stay better.

    For *L2* cache, *avx* is a little bit better than *sse*.

    Other memory space have not significant.

*** load

    #+CAPTION: Load benchmark on an Intel Core i5-8400
    #+NAME: fig:load_INTEL_CORE_I5-8400
    #+ATTR_HTML: :width 1500px
    [[./coffee_lake/benchmark/load/load_bw.png]]

    We can see a difference between all size for all cache. *avx*
    still better.

*** memcpy

    #+CAPTION: Memcpy benchmark on an Intel Core i5-8400
    #+NAME: fig:load_INTEL_CORE_I5-8400
    #+ATTR_HTML: :width 1500px
    [[./coffee_lake/benchmark/memcpy/memcpy_bw.png]]

    For *L1* cache, the bandwith is not really stable.

    But for other memory space it is.

    We can however raise that we have this list of memory space
    speed :
    1. L1 cache
    2. L2 cache
    3. L3 cache
    4. DRAM

*** ntstore

    #+CAPTION: Non temporal store benchmark on an Intel Core i5-8400
    #+NAME: fig:load_INTEL_CORE_I5-8400
    #+ATTR_HTML: :width 1500px
    [[./coffee_lake/benchmark/ntstore/ntstore_bw.png]]

    All memory space are relatively the same. That is good. In average
    35 GiB/s.

*** pc

    #+CAPTION: Pointer chasing benchmark on an Intel Core i5-8400
    #+NAME: fig:load_INTEL_CORE_I5-8400
    #+ATTR_HTML: :width 1500px
    [[./coffee_lake/benchmark/pc/pc_bw.png]]

    For all cache the bandwith was the same. I mean that they are
    relatively near in bandwith in general.

    But I don't undestand the DRAM bandwith.

*** reduc

    #+CAPTION: Reduction benchmark on an Intel Core i5-8400
    #+NAME: fig:load_INTEL_CORE_I5-8400
    #+ATTR_HTML: :width 1500px
    [[./coffee_lake/benchmark/reduc/reduc_bw.png]]

    We have better badnwith for *avx* with all cache.

    For *DRAM* we have a problem that I don't understand.

*** store

    #+CAPTION: Store benchmark on an Intel Core i5-8400
    #+NAME: fig:load_INTEL_CORE_I5-8400
    #+ATTR_HTML: :width 1500px
    [[./coffee_lake/benchmark/store/store_bw.png]]

    We double on bandwith for L1 cache between *sse* and *avx*.

    For other memory space, that is relatively the same between both.

** Ivy Bridge
*** Information

    I run all benchmark on a *Native Linux* with *Manjaro i3*.

    #+CAPTION: Topology of Intel Pentium 2117U
    #+NAME: fig:topology_INTEL_PENTIUM_2117U
    #+ATTR_HTML: :width 1500px
    [[./ivy_bridge/system/topology/lstopo.png]]

    Caches size:
    - L1: 32K
    - L2: 256K
    - L3: 2048K
    - DRAM: 4G

    I decide to run benchmarks with :
    - L1: 24K
    - L2: 200K
    - L3: 1000K
    - DRAM: 4000K

    This size are chosen because they fit on their caches/ram.

    The governor was set to *performance*, because *userspace* wasn't
    available.
    And the frequency was set to *1.80GHz*. (the maximum)

    *IMPORTANT NOTE*: I have not *AVX* on this machine, so *pc* and
    *dotprod* benchmarks was not run.

**** Command

#+begin_src bash
$ ./install.sh ivy_bridge
$ ./compile.sh
$ sudo cpupower -c 1 frequency-set --governor performance
$ ./run.sh ivy_bridge 1 24 200 1000 4000
$ ./plot.sh 10 ivy_bridge "Intel Pentium 2117U"
#+end_src

*** copy

    #+CAPTION: Copy benchmark on an Intel Pentium 2117U
    #+NAME: fig:load_INTEL_PENTIUM_2117U
    #+ATTR_HTML: :width 1500px
    [[./ivy_bridge/benchmark/copy/copy_bw.png]]

    We have only a difference for *L1* cache.

*** dotprod

    #+CAPTION: Dotprod benchmark on an Intel Pentium 2117U
    #+NAME: fig:load_INTEL_PENTIUM_2117U
    #+ATTR_HTML: :width 1500px
    [[./ivy_bridge/benchmark/dotprod/dotprod_bw.png]]

    No *AVX*.

*** load

    #+CAPTION: Load benchmark on an Intel Pentium 2117U
    #+NAME: fig:load_INTEL_PENTIUM_2117U
    #+ATTR_HTML: :width 1500px
    [[./ivy_bridge/benchmark/load/load_bw.png]]

    We have only a difference for *L1* cache.

*** memcpy

    #+CAPTION: Memcpy benchmark on an Intel Pentium 2117U
    #+NAME: fig:load_INTEL_PENTIUM_2117U
    #+ATTR_HTML: :width 1500px
    [[./ivy_bridge/benchmark/memcpy/memcpy_bw.png]]

    For *L1* cache, the bandwith is not really stable.

    But for other memory space it is.

    We can however raise that we have this list of memory space
    speed :
    1. L1 cache
    2. L2 cache
    3. L3 cache
    4. DRAM

*** ntstore

    #+CAPTION: Non temporal store benchmark on an Intel Pentium 2117U
    #+NAME: fig:load_INTEL_PENTIUM_2117U
    #+ATTR_HTML: :width 1500px
    [[./ivy_bridge/benchmark/ntstore/ntstore_bw.png]]

    All memory space are relatively the same. That is good. In average
    9 GiB/s.

*** pc

    #+CAPTION: Pointer chasing benchmark on an Intel Pentium 2117U
    #+NAME: fig:load_INTEL_PENTIUM_2117U
    #+ATTR_HTML: :width 1500px
    [[./ivy_bridge/benchmark/pc/pc_bw.png]]

    No *avx*.

*** reduc

    #+CAPTION: Reduction benchmark on an Intel Pentium 2117U
    #+NAME: fig:load_INTEL_PENTIUM_2117U
    #+ATTR_HTML: :width 1500px
    [[./ivy_bridge/benchmark/reduc/reduc_bw.png]]

    For all cache, we have a difference between all size.

*** store

    #+CAPTION: Store benchmark on an Intel Pentium 2117U
    #+NAME: fig:load_INTEL_PENTIUM_2117U
    #+ATTR_HTML: :width 1500px
    [[./ivy_bridge/benchmark/store/store_bw.png]]

    For all memory space, it is relatively the same between all size.
