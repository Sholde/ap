#+title: Sequetial Vector Benchmark
#+author: Sholde
#+date: 2021

* Script
** Installation

   The =install.sh= script provide all information about the system.

   Take *1 argument*:
   - cpu architecture, to create a directory for follow information.

   In the following lines, we supposed that we are on
   =cpu_architecture/= directory.

   Compiler information are on =compiler/= directory.
   See =compiler/gcc_version.txt=.

   *glibc* information are on =lib/= directory.
   See =lib/glibc_version.txt=.

   *cpu*, *caches* and *memory* information are on =system/=
   directory.
   - dmidecode: =system/hw.txt=
   - cpu: =system/cpu/info.txt=
   - caches: =system/caches/all.txt=
   - memory: =system/memory/info.txt=
   - numa: =system/numa/info.txt=
   - lstopo: =system/topo/lstopo.png=

** Compilation
   
   The =compile.sh= compile all programs.
   Executables are in their source directory.

** Execution
   
   The =run.sh= execute all programs and store their output on file.

   It take *exactly* 6 arguments in entry:
   1. cpu architecture (to create a folder and store data)
   2. core id (taskset the program on this core)
   3. size to fit in L1 cache
   4. size to fit in L2 cache
   5. size to fit in L3 cache
   6. size to fit in DRAM

*** More details about the script

    After checking arguments, run all programs by number of array
    they use. Here we are at most *2 arrays*. Therefore the script
    run programs which use *1 array*, then programs use *2 arrays*.
    Run all benchmark with 1,000 iteration for more accuracy.

    Store all output in the directory: =cpu_architecture/benchmark/=.
    _For example:_ if we specify *knl* cpu architecture, the *reduc*
    benchmark output will be on =knl/benchmark/reduc/= directory.

    *IMPORTANT NOTE:* It store *BRUT* output in *.txt* files and
    plotting data in *.dat* files.

    We can easily see the standart derivation and others information
    of all benchmark by *cat* the file.

** Plotting

   Plotting step are separate in 2 files:
   - a bash file
   - a gnuplot file

*** Bash file

    Main plotting script.
    
    Take *3 arguments*:
    1. a delta (to avoid plotting with max y)
    2. cpu architecture (without spaces)
    3. the name of cpu


    Plotting into *png* file on the same directory of benchmark data.

*** GNUplot file

    Not supposed to be called, but you can. The bash script above call
    this *gnuplot* script.

    _WARNING_: it return a *pdf* file on output.

    Take *4 arguments*:
    1. a delta (to avoid plotting with max y)
    2. path of data file + name of benchmark (i.e: knl/benchmark/reduc/reduc)
    3. the name of benchmark (without spaces)
    4. the name of cpu

    _NOTE:_ The *gnuplot* script take *the higher y of L1 cache
    as reference*.

** Check standart derivation

   The script =check_sd.sh= print all standard derivation of given cpu
   architecture benchmark.

   Take *1 argument*:
   - cpu architecture, to print all standart derivation

   We can easily see if standart derivation is under *7%*.

** Clean

   Clean the follow list of file :
   - temporary file (*~)
   - executable file

* Benchmark
** Coffee Lake
*** Information

    I run all benchmark on a *USB LIVE Manjaro XFCE* with 4G.

    #+CAPTION: Topology of Intel Core i5-8400
    #+NAME: fig:topology_INTEL_CORE_I5-8400
    #+ATTR_HTML: :width 1500px
    [[./coffee_lake/system/topology/lstopo.png]]

    Caches size:
    - L1: 32K
    - L2: 256K
    - L3: 9216K
    - DRAM: 8G

    I decide to run benchmarks with :
    - L1: 24K
    - L2: 200K
    - L3: 5000K
    - DRAM: 15000K

    This size are chosen because they fit on their caches/ram.

    The governor was set to *performance*, because *userspace* wasn't
    available. I try to disable *intel_pstate* on */etc/default/grub*
    but when I update grub it fail. I think is due to the *USB LIVE*.
    I try also to install grub but it fail again.

    _cpu max frequency:_ 4.00GHz
    _cpu frequency:_ ~ 4.00GHz (because governor = performance)

    All program are set to core 5. I have 6 core.

**** Command

#+begin_src bash
$ ./install.sh coffee_lake
$ ./compile.sh
$ sudo cpupower -c 5 frequency-set --governor performance
$ ./run.sh coffee_lake 5 24 200 5000 15000
$ ./plot.sh 10 coffee_lake "Intel Core i5-8400"
#+end_src

*** copy

    #+CAPTION: Copy benchmark on an Intel Core i5-8400
    #+NAME: fig:load_INTEL_CORE_I5-8400
    #+ATTR_HTML: :width 1500px
    [[./coffee_lake/benchmark/copy/copy_bw.png]]

    We can see a difference of bandwith for *SSE* and *AVX* for *L1
    cache*, where *AVX* is better but for other *memory spaces* there are
    not difference.

    Also, all *sse* size have the same bandwith.

*** dotprod

    #+CAPTION: Dotprod benchmark on an Intel Core i5-8400
    #+NAME: fig:load_INTEL_CORE_I5-8400
    #+ATTR_HTML: :width 1500px
    [[./coffee_lake/benchmark/dotprod/dotprod_bw.png]]

    For *L1* cache we can see an evolution between all size for *SSE*
    and AVX*, but *AVX* stay better, we have an huge difference (2
    times faster). Also for *L2* cache *AVX* is better.

    For *L3* cache, *AVX* is a little bit better than *SSE*.

    The difference for *DRAM* are not significant.

*** load

    #+CAPTION: Load benchmark on an Intel Core i5-8400
    #+NAME: fig:load_INTEL_CORE_I5-8400
    #+ATTR_HTML: :width 1500px
    [[./coffee_lake/benchmark/load/load_bw.png]]

    We can see a difference between all size for all cache.

    *AVX* still better than *SSE*.

*** memcpy

    #+CAPTION: Memcpy benchmark on an Intel Core i5-8400
    #+NAME: fig:load_INTEL_CORE_I5-8400
    #+ATTR_HTML: :width 1500px
    [[./coffee_lake/benchmark/memcpy/memcpy_bw.png]]

    For *L1* cache, the bandwith is not really stable.

    But for other memory space it is.

    We can however raise that we have this list of memory space
    speed :
    1. L1 cache
    2. L2 cache
    3. L3 cache
    4. DRAM

    And we can note that we have a ratio of 2 between all cache.

*** ntstore

    #+CAPTION: Non temporal store benchmark on an Intel Core i5-8400
    #+NAME: fig:load_INTEL_CORE_I5-8400
    #+ATTR_HTML: :width 1500px
    [[./coffee_lake/benchmark/ntstore/ntstore_bw.png]]

    All memory space are relatively the same. That is good. In average
    35 GiB/s.

*** pc

    #+CAPTION: Pointer chasing benchmark on an Intel Core i5-8400
    #+NAME: fig:load_INTEL_CORE_I5-8400
    #+ATTR_HTML: :width 1500px
    [[./coffee_lake/benchmark/pc/pc_bw.png]]

    _NOTE:_ I replot manually with the *gnuplot* script with a delta
    of 40 because the *gnuplot* script take *the higher y of L1 cache
    as reference*.

    For all cache the bandwith was the same. I mean that they are
    relatively near in bandwith in general.

    But I don't undestand the DRAM bandwith.

*** reduc

    #+CAPTION: Reduction benchmark on an Intel Core i5-8400
    #+NAME: fig:load_INTEL_CORE_I5-8400
    #+ATTR_HTML: :width 1500px
    [[./coffee_lake/benchmark/reduc/reduc_bw.png]]

    We have better bandwith for *AVX* with all cache, and we keep an
    evolution between all size.

    For *DRAM* we have a problem that I don't understand.

*** store

    #+CAPTION: Store benchmark on an Intel Core i5-8400
    #+NAME: fig:load_INTEL_CORE_I5-8400
    #+ATTR_HTML: :width 1500px
    [[./coffee_lake/benchmark/store/store_bw.png]]

    We double on bandwith for *L1* cache between *SSE* and *AVX*.

    For *L2* cache *AVX* is a half faster than *SSE*.

    For other memory spaces, that is relatively the same between both.

    We can raise that all size of *SSE* and *AVX* give the same bandwith.

** Ivy Bridge
*** Information

    I run all benchmark on a *Native Linux* with *Manjaro i3*.

    #+CAPTION: Topology of Intel Pentium 2117U
    #+NAME: fig:topology_INTEL_PENTIUM_2117U
    #+ATTR_HTML: :width 1500px
    [[./ivy_bridge/system/topology/lstopo.png]]

    Caches size:
    - L1: 32K
    - L2: 256K
    - L3: 2048K
    - DRAM: 4G

    I decide to run benchmarks with :
    - L1: 24K
    - L2: 200K
    - L3: 1000K
    - DRAM: 4000K

    This size are chosen because they fit on their caches/ram.

    The frequency was set to *1.80GHz*. (the maximum)

    All program are set to core 1. I have 2 core.

    *IMPORTANT NOTE*: I have not *AVX* on this machine, so *pc* and
    *dotprod* benchmarks was not run.

**** Command

#+begin_src bash
$ ./install.sh ivy_bridge
$ ./compile.sh
$ sudo cpupower -c 1 frequency-set -f 1.8GHz
$ ./run.sh ivy_bridge 1 24 200 1000 4000
$ ./plot.sh 10 ivy_bridge "Intel Pentium 2117U"
#+end_src

*** copy

    #+CAPTION: Copy benchmark on an Intel Pentium 2117U
    #+NAME: fig:load_INTEL_PENTIUM_2117U
    #+ATTR_HTML: :width 1500px
    [[./ivy_bridge/benchmark/copy/copy_bw.png]]

    We have only a difference for *L1* cache between all size.

    *L1* and *L2* have relatively the same bandwith.

*** dotprod

    #+CAPTION: Dotprod benchmark on an Intel Pentium 2117U
    #+NAME: fig:load_INTEL_PENTIUM_2117U
    #+ATTR_HTML: :width 1500px
    [[./ivy_bridge/benchmark/dotprod/dotprod_bw.png]]

    No *AVX*.

*** load

    #+CAPTION: Load benchmark on an Intel Pentium 2117U
    #+NAME: fig:load_INTEL_PENTIUM_2117U
    #+ATTR_HTML: :width 1500px
    [[./ivy_bridge/benchmark/load/load_bw.png]]

    We have only a difference for *L1* cache for all size.

*** memcpy

    #+CAPTION: Memcpy benchmark on an Intel Pentium 2117U
    #+NAME: fig:load_INTEL_PENTIUM_2117U
    #+ATTR_HTML: :width 1500px
    [[./ivy_bridge/benchmark/memcpy/memcpy_bw.png]]

    We can raise that we have this list of memory space
    speed :
    1. L1 cache
    2. L2 cache
    3. L3 cache
    4. DRAM

*** ntstore

    #+CAPTION: Non temporal store benchmark on an Intel Pentium 2117U
    #+NAME: fig:load_INTEL_PENTIUM_2117U
    #+ATTR_HTML: :width 1500px
    [[./ivy_bridge/benchmark/ntstore/ntstore_bw.png]]

    All memory spaces are relatively the same. That is good. In average
    9 GiB/s.

*** pc

    #+CAPTION: Pointer chasing benchmark on an Intel Pentium 2117U
    #+NAME: fig:load_INTEL_PENTIUM_2117U
    #+ATTR_HTML: :width 1500px
    [[./ivy_bridge/benchmark/pc/pc_bw.png]]

    No *AVX*.

*** reduc

    #+CAPTION: Reduction benchmark on an Intel Pentium 2117U
    #+NAME: fig:load_INTEL_PENTIUM_2117U
    #+ATTR_HTML: :width 1500px
    [[./ivy_bridge/benchmark/reduc/reduc_bw.png]]

    For all cache, we have a difference between all size. A little bit
    difference for *DRAM*.

*** store

    #+CAPTION: Store benchmark on an Intel Pentium 2117U
    #+NAME: fig:load_INTEL_PENTIUM_2117U
    #+ATTR_HTML: :width 1500px
    [[./ivy_bridge/benchmark/store/store_bw.png]]

    For all memory space, it is relatively the same between all
    size. We also see the memory spaces speed list :
    1. L1 cache
    2. L2 cache
    3. L3 cache
    4. DRAM
